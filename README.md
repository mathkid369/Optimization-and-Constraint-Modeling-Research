# Optimization-and-Constraint-Modeling-Research
Most of the existing research in constraint model generation is focused on fine-tuning language models or using reasoning systems to promote structured reasoning. This project addresses this issue by taking existing constraint modeling and optimization datasets as a foundation and implementing RAG to strengthen the reasoning process of the model. 

Our approach allows for benchmarking and fine-tuning LLMs in complex reasoning tasks within optimization and constraint modeling research. Our data generation process follows a simple structured pipeline of Description to Persona to Problem to Data through open source LLMs. The descriptions were extracted from the Text2Zinc dataset and were first transformed into detailed professional personas using techniques from the research paper “Using AI for User Representation: An Analysis of 83 Personas” and "Scaling Synthetic Data Creation with 1,000,000,000 Personas" paper. Each persona has a realistic occupation and context that naturally relates to a optimization modeling problem with constraints. 

Through experimentation, we found that generating personas three at a time yielded significantly higher quality and context than generating them all at once. We applied the same method to generate the problems from the personas, ensuring that each problem was unique, challenging, and grounded in the persona’s perspective.

Once the personas and problems were created, I structured each instance into a JSON file containing the persona, the problem statement, decision variables, constraints, and the objective function. I have also generated full python solutions to each of these problems which I will need to validate using Gurobi. These structured instances will be converted to vector embeddings and stored in a vector database to be then used within a RAG framework to increase the reasoning capabilities of the model. Future work will focus on embedding the dataset into a vector database, refining the RAG pipeline, and conducting experiments to benchmark the model performance against other baseline and fine-tuned models such as GPT-4, Deepseek-V3, and OptMATH.
