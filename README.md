# Optimization-and-Constraint-Modeling-Research
Most of the existing research in constraint model generation is focused on fine-tuning language models or using reasoning systems to promote structured reasoning. This project addresses this issue by taking existing constraint modeling and optimization datasets as a foundation and implementing RAG to strengthen the reasoning process of the model. 

Our approach allows for benchmarking and fine-tuning LLMs in complex reasoning tasks within optimization and constraint modeling research. Our data generation process follows a simple structured pipeline of Description to Persona to Problem to Data through open source LLMs. The descriptions were extracted from the Text2Zinc dataset and were first transformed into detailed professional personas using techniques from the research paper “_Using AI for User Representation: An Analysis of 83 Personas_” and "_Scaling Synthetic Data Creation with 1,000,000,000 Personas_" paper. 

Each persona has a realistic occupation and context that naturally relates to a optimization modeling problem with constraints. It includes domain specific context and a clear link between the person’s role and the mathematical or operational challenge they face. Here is an example of a persona:

_Working as a lighting systems engineer at a civil infrastructure firm, this professional specializes in designing roadway lighting that meets both safety standards and energy-efficiency regulations. They oversee computational models that simulate light distribution based on lamp positioning and power outputs. When a new urban renewal project required precise illumination uniformity across complex street geometries, they turned to numerical optimization methods to calibrate lamp powers against desired brightness targets. This problem directly mirrors the illumination optimization model, reflecting their responsibility to minimize energy waste while achieving required photometric performance._

Through experimentation, we found that generating personas three at a time yielded significantly higher quality and context than generating them all at once. We applied the same method to generate the problems from the personas, ensuring that each problem was unique, challenging, and grounded in the persona’s perspective.

Once the personas and problems were created, I structured each instance into a JSON file containing the persona, the problem statement, decision variables, constraints, and the objective function. I have also generated full python solutions to each of these problems which I will need to validate using Gurobi. These structured instances will be converted to vector embeddings and stored in a vector database to be then used within a RAG framework to increase the reasoning capabilities of the model. Our future work will focus on embedding the dataset into a vector database, refining the RAG pipeline, and conducting experiments to benchmark the model performance against other baseline and fine-tuned models such as GPT-4, Deepseek-V3, and OptMATH.
